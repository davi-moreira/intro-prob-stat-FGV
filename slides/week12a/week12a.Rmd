---
title: "Week 12a - Uncertainty"
subtitle: "Standard Errors, P-Values, and Hypothesis Test<html><div style='float:left'></div><hr color='#EB811B' size=1px width=800px></html>"
author: "Umberto Mignozzetti"
date: "June 02"
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature: 
      beforeInit: "macros.js"
      highlightStyle: github
      ratio: 4:3
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
r <- getOption("repos")
r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
options(repos = r)
set.seed(12345)
```

<style>

.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 6px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: #EB811B;
}

.orange {
  color: #EB811B;
}
</style>

# Today's Agenda

.font150[
* Confidence Interval

* Null and alternative hypothesis

* One-sample tests

* Two-sample tests
]
---

# Confidence Intervals

.font130[
* $\alpha$ is the probability of rejecting the null hypothesis when it is in fact true

* Select a value for $\alpha$, usually 0.05. 

* So we expect every 1 in 20 (5/100) confidence intervals to be false if we use $\alpha = 0.05$

* Then, we select the _critical value_: $z = 1 - \alpha$. This is a point of the test distribution (usually the normal) that determines whether to reject the null hypothesis

* $z$ is the number of standard deviations of the distribution

* We usually use 1.96, often rounded to 2
]
---

# Confidence Intervals

.font150[
* The confidence interval you will see most often is this one: 

$$CI(1-\alpha) = [\overline{X} - 1.96 \times SE, \overline{X} + 1.96 \times SE]$$

* Why? Beause if you take the mean of the normal distribution and add $\pm 2 \times SE$ you will cover 95% of the area

* .orange[When evaluating effects, we usually judge them based on whether the 95% confidence interval covers zero or not]

* If it covers zero, there is a possibility that the effect is, well, zero. Zero effect = no relationship between variables
]
---

# Average Treatment Effects

.font150[
* How do we estimate the ATE in RTCs?

* Difference in means between treatment and control group

* Sample average in treated group $\overline{X}_{t}$ and control group $\overline{X}_{c}$

* SE treated: $\hat{SE}_{t} = \sqrt{\frac{\hat{\sigma}^{2}_{t}}{N_{t}}}$

* SE control: $\hat{SE}_{c} = \sqrt{\frac{\hat{\sigma}^{2}_{c}}{N_{c}}}$

* What do we use for $\sigma^2$? The variance of the sample, as we don't have the variance of the full sampling distribution

]
---

# Average Treatment Effects

.font150[
* We can use these SEs to construct confidence intervals around each of the averages

* But we need to construct SEs _around the difference in means itself_ $(\overline{X}_{t} - \overline{X}_{c})$

* $\hat{SE}_{t-c} = \sqrt{\frac{\mathbb{V}(X_{t})}{N_{t}} + \frac{\mathbb{V}(X_{c})}{N_{c}}}$
]
---

# Average Treatment Effects

.font140[
* We can use the standard error to construct a 95% confidence interval for the difference in means:

* Example: ATE = 3.5, SE = 2.65

* What is the confidence interval?
]
--
.font140[
* $CI(0.95) = [3.5 - 1.96 \times 2.65, \ 3.5 + 1.96 \times 2.65]$

* $CI(0.95) = [-1,694, \ 8.694]$

* Is the difference statistically significant?
]
--
.font140[
* _No_, as the difference between the treatment and control groups can be zero
]
---

# The summary function in R

.font150[
* R has a convenient function to see if the results are statistically significant 

* `summary(name_of_your_model)`

* It shows the coefficient, the standard error, and the significance level all at once
]

---

# Example - Resume

```{r resume01,fig.align="center",tidy=F,warning=F,message=F,cache=T}
resume <- read.csv("https://raw.githubusercontent.com/umbertomig/intro-prob-stat-FGV/master/datasets/resume.csv")
model1 <- lm(call ~ race, data = resume)
summary(model1)
```
---

# Example - Resume

```{r resume02,fig.align="center",tidy=F,warning=F,message=F,cache=T}
model2 <- lm(call ~ sex, data = resume)
summary(model2)
```
---
# Example - Resume

```{r resume03,fig.align="center",tidy=F,warning=F,message=F,cache=T}
model3 <- lm(call ~ race + sex, data = resume)
summary(model3)
```
---

# Confidence Intervals

.font150[
* To recap:

* When you're reading articles or writing your own, there's an easy way to see if the effect is statistically significant with $p-value < 0.05$

* Only three steps:

  - Check the model coefficient
  - Check the standard error and multiply it by two
  - Sum and subtract the value from the coefficient. If it gives you zero, it's not significant. If it doesn't include zero, it is statistically significant
]
---

# Hypothesis Testing

.font150[
* Goal: Try to determine whether result is due to chance or not

* Method: "Proof" by contradiction

* We try to reject the _null hypothesis_

  - _X_ is uncorrelated with/has no effect on _Y_
  - _X_ is no different from _Y_
]
---

# Proof by Contradiction

.font140[
* Formulated by Aristotle

* Starts with the _law of the excluded middle_: either a preposition is true, or its opposite is true

* $\forall P \vdash (P \lor \lnot P)$ 
]
--

.font140[
* First, we make a statement _P_, which we believe is true

* Then, we assume _P_ is false

* Lastly, we prove _P_ cannot be false given our data/reasoning

* Thus, _P_ must be true
]
---

# Hypothesis Testing

.font150[
* With statistics, we can reject the null hypothesis with 100% confidence

* Why? Because statistics is _probabilistic_

* So we use a probabilistic version of proof by contradiction
]
---

# Hypothesis Testing

.font150[
* We construct the _null hypothesis_ $\rightarrow H_0$ (what we want to refute), and the _alternative hypothesis_ $\rightarrow H_1$
 
* We select a test statistic $T$

* Figure out the sampling distribution of $T$ under $H_0$

* Is the observed value of $T$ likely to occur under $H_0$?

  - Yes - fail to reject $H_0$
  - No - reject $H_0$
]

---

# P-Value

.font140[
* .orange[p-value] is the probability, computed under $H_0$, of observing a value ofthe test statistic at least as extreme as its observed value

* A smaller p-value presents stronger evidence against $H_0$

* p-value less than $\alpha$ indicates statistical significance

* $\alpha$ is usually 0.05

* .orange[Remember:] p-value is NOT the probability that $H_0$ $(H_1)$ is true (false)

* Statistical significance does not necessarily imply scientific significance
]
---

# One-Sample Test - Continuous Variable

.font150[
* We can also test if $\mu$ from a sample corresponds to a value we are interested in

* For example: you run a factory and want to try a new machine. Does the machine actually improve your results, or are the results just due to chance?

* R function: `t.test()`
]
---

# One-Sample Test - Continuous Variable

.font150[
* Example: 

* A bottle filling machine is set to fill bottles with soft drink to a volume of 500 ml. The actual volume is known to follow a normal distribution. The manufacturer believes the machine is under-filling bottles. A sample of 20 bottles is taken and the volume of liquid inside is measured.
]
---

# One-Sample Test - Continuous Variable

.font110[
```{r bottles01,fig.align="center",tidy=F,warning=F,message=F,cache=T}
bottles <- c(484.11, 459.49, 471.38, 512.01, 494.48, 528.63, 493.64,
             485.03, 473.88, 501.59, 502.85, 538.08, 465.68, 495.03,
             475.32, 529.41, 518.13, 464.32, 449.08, 489.27)

mean(bottles)

t.test(bottles, mu = 500)
```
]
---

# Two-Sample Test

.font150[
* We can also test whether two different samples have the same $\mu$

* This is particularly useful for randomised experiments

* We test whether the treatment and control groups have similar means

* If so, then we conclude the treatment likely doesn't have any effect

* We use the same `t.test()` function 
]
---

# Two-Sample Test

.font120[
```{r two01,fig.align="center",tidy=F,warning=F,message=F,cache=T}
call_blacks <- subset(resume$call, resume$race == "black") 
call_whites <- subset(resume$call, resume$race == "white") 

t.test(call_blacks, call_whites)
```
]
---

# t.test() or lm()?

.font110[
```{r two02,fig.align="center",tidy=F,warning=F,message=F,cache=T}
summary(lm(call ~ race, data = resume))
```
]
---

# t.test() or lm()?

.font150[
* Both produce exactly the same results

* The intercept in the linear model is the mean of the control group (when all other variables are zero)

* The coefficient is the average for the treatment group

* I suggest you to use `lm()`: you can add control variables, interactions, etc
]

---

class: inverse, center, middle

# Questions?

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html>  

---

class: inverse, center, middle

# See you next week!

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=720px></html>  
